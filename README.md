# ATIoT-Fine-Tuning
Performing Fine-Tuning of LLM with ATIoT tool logic. The original ATIoT tool can be found [here](https://github.com/Inthen/ATIoT). The purpose of this work is to verify if fine-tuning an LLM can make the model learn the logic of the original tool. 

# Datasets
The datasets created for the fine-tuning process can be found [here](https://huggingface.co/datasets/Inthen/ATIoT).

# Instructions
The main .py file was prepared to run in a Google Colab notebook. Experimental results show a minimum of 32GB of VRAM required to perform the fine-tuning process. Login token for Hugging Face must be added to the file.

# Acknowledgements

This work was performed under the scope of Project SECURIoTESIGN, with funding from FCT/COMPETE/ FEDER (projects with reference numbers UIDB/50008/2020 and POCI-01-0145-FEDER-030657). This work is funded by Portuguese FCT/MCTES through national funds and, when applicable, co-funded by EU funds under the project UIDB/50008/2020, and FCT research and doctoral grants BIM/nº32/2018-B00582 and SFRH/BD/133838/2017, respectively, and also supported by operation Centro-01-0145-FEDER-000019 - C4 - Centro de Competências em Cloud Computing, co-financed by the European Regional Development Fund (ERDF) through the Programa Operacional Regional do Centro (Centro 2020), in the scope of the Sistema de Apoio à Investigação Científica e Tecnológica - Programas Integrados de IC&DT.
